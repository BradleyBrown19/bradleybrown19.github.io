<!DOCTYPE HTML>
<html lang="en"><head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5BKCJXHXQH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5BKCJXHXQH');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bradley Brown</title>
  
  <meta name="author" content="Bradley Brown">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style type="text/css">
    /* latin-ext */
    @font-face {
      font-family: 'Lato';
      font-style: italic;
      font-weight: 400;
      src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
      font-family: 'Lato';
      font-style: italic;
      font-weight: 400;
      src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
    /* latin-ext */
    @font-face {
      font-family: 'Lato';
      font-style: italic;
      font-weight: 700;
      src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
      font-family: 'Lato';
      font-style: italic;
      font-weight: 700;
      src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
    /* latin-ext */
    @font-face {
      font-family: 'Lato';
      font-style: normal;
      font-weight: 400;
      src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
      font-family: 'Lato';
      font-style: normal;
      font-weight: 400;
      src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
    /* latin-ext */
    @font-face {
      font-family: 'Lato';
      font-style: normal;
      font-weight: 700;
      src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
      font-family: 'Lato';
      font-style: normal;
      font-weight: 700;
      src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }

    .phone-teasers {
      display: none;
    }

    @media only screen and (max-width: 768px) {
      .abstract {
        display: none;
      }

      .comp-teasers {
        display: none;
      }

      .phone-teasers {
        display: block;
      }
    }

  </style>

  <!-- <link rel="stylesheet" type="text/css" href="stylesheet.css"> -->
  <link rel="icon" class="top-icon" type="image/png" href="images/icon.png">

  <!-- /* Random icon selection */ -->
  <script>
    /* // Define an array of available icons */
    const icons = [
      "images/icons/sunglasses.png",
      "images/icons/computer.png",
      "images/icons/cowboy.png",
      "images/icons/smile.png"
    ];

    /* // Select a random icon from the array */
    const randomIcon = icons[Math.floor(Math.random() * icons.length)];

    /* // Replace the icon's URL in the HTML code */
    const iconElement = document.querySelector(".top-icon");

    iconElement.href = randomIcon;
  </script>

</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bradley Brown</name>
              </p>
              <p>
                Hello! I am a third-year PhD student, previously at Oxford and now at Stanford. I am <i>broadly</i> interested in research addressing gaps in models that prevent them from being applied to currently out-of-reach real-world tasks. This includes improved long-context understanding and data efficiency, methods that allow models to continually learn from new experiences, and architectures whose capability scales better with test-time compute. Currently, I am researching how we can train natively parallel reasoning models with RL.
              </p>
              
              <p>
                Recently, I've also worked on methods for improving LLMs by scaling test-time compute 
                (<a target="_blank" href="https://arxiv.org/abs/2407.21787">Large Language Monkeys</a>, 
                <a target="_blank" href="https://arxiv.org/abs/2501.14723">CodeMonkeys</a>), as well as 
                corresponding systems research to make these models more efficient 
                (<a target="_blank" href="https://arxiv.org/abs/2402.05099">Hydragen</a>).
              </p>
              
              <p>
                Before this, my research focused on multimodal deep learning, including building one of the first 
                generative models of open-world 3D scenes 
                (<a target="_blank" href="https://arxiv.org/abs/2304.09787">NF-LDM</a>). I have also worked on more theoretical 
                projects exploring how the geometric structure of image data influences model performance 
                (<a target="_blank" href="https://arxiv.org/abs/2207.02862">Union of Manifolds</a>, 
                <a target="_blank" href="https://arxiv.org/abs/2211.13239">Geometry of Activations</a>).
              </p>
                
              <p>
                I have been fortunate to work with many amazing collaborators. At Stanford, I am working in the 
                <a target="_blank" href="https://scalingintelligence.stanford.edu/">Scaling Intelligence Lab</a> 
                with 
                <a target="_blank" href="http://azaliamirhoseini.com/">Professor Azalia Mirhoseini</a>. At the University of Oxford, I was supervised by 
                <a target="_blank" href="https://ronnie-clark.co.uk/">Professor Ronald Clark</a> in the 
                <a target="_blank" href="https://pixl.cs.ox.ac.uk/">PiXL</a> group. Before this, I obtained my 
                undergraduate degree at the University of Waterloo, where I studied Software Engineering with a joint major in Combinatorics and Optimization. Through Waterloo's co-op program, I completed six internships primarily focused on AI research across a variety of domains.
                This includes 3D generative model research at <a target="_blank" href="https://research.nvidia.com/labs/toronto-ai/">Nvidia's Toronto AI Lab</a> 
                advised by <a target="_blank" href="https://www.cs.utoronto.ca/~fidler/">Professor Sanja Fidler</a>, theoretical and recommender system research at  
                <a target="_blank" href="https://layer6.ai/">Layer 6 AI</a>, and computer vision research at Akasha Imaging (acquired by Intrinsic).
              </p>

              <p style="text-align:center">
                    [
                <a target="_blank" href="mailto:bradley19brown@gmail.com">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=TNoWMVEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://twitter.com/brad19brown">Twitter</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/bradleycabrown/">LinkedIn</a>
]
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/brad.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <p> 
              </p>
              <p>
              </p>
        <!-- <h2>News</h2>

            <ul>
                <li><b>June 2024</b>: Starting as a Visiting Student Researcher in the <a target="_blank" href="https://scalingintelligence.stanford.edu//">Scaling Intelligence Lab</a> at Stanford with Professor Azalia Mirhoseini!</li>
                <li><b>April 2023</b>: Paper @ CVPR 2023 - Our work creating a generative model of open-world 3D scenes, <a target="_blank" href="https://arxiv.org/abs/2304.09787">NF-LDM</a>, was accepted to <a target="_blank" href="https://cvpr2023.thecvf.com/">CVPR 2023</a>!</li>
                <li><b>April 2023</b>: Tiny Paper @ ICLR 2023 - Our work <a target="_blank" href="https://openreview.net/forum?id=GJhsHNKm7kj">examing a limitation of increased scale in LLMs</a> was accepted to the <a target="_blank" href="https://iclr.cc/">ICLR 2023</a> tiny paper track.</li>
                <li><b>January 2023</b>: Paper @ ICLR 2023 - Our work <a target="_blank" href="https://arxiv.org/abs/2207.02862">Verifying the Union of Manifolds Hypothesis for Image Data</a> was accepted to <a target="_blank" href="https://iclr.cc/">ICLR 2023</a>!</li>
                <li><b>October 2022</b>: Workshop Paper @ NeurIPS 2022 - Our work <a target="_blank" href="https://arxiv.org/abs/2211.13239">Relating Regularization and Generalization through the Intrinsic Dimension of Activations</a> was accepted to <a target="_blank" href="https://opt-ml.org/">OPT 2022</a> and <a target="_blank" href="https://hity-workshop.github.io/NeurIPS2022/">HITY 2022</a>. </li> 
                <li><b>September 2022</b>: <b>Internship</b> - Returning to NVIDIA as a Research Scientist intern in the <a target="_blank" href="https://nv-tlabs.github.io/">Toronto AI lab</a></li>
                <li><b>September 2022</b>: RecSys Competion - Our team placed 2 / 303 teams in the <a target="_blank" href="https://recsys.acm.org/recsys22/challenge/">2022 RecSys Challenge</a>, read about our approach <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3556702.3556844">here</a>!</li> 
                <li><b>February 2022</b>: <b>Internship</b> - Joined <a target="_blank" href="https://layer6.ai/">Layer 6 AI</a> as a Machine Learning Researcher in their generative modelling team.</li>
                <li><b>May 2021</b>: <b>Internship</b> - Joined NVIDIA as a Research Scientist intern in the <a target="_blank" href="https://nv-tlabs.github.io/">Toronto AI lab</a>.</li>
                <li><b>July 2021</b>: Paper @ ICCV 2021 - Our work <a target="_blank" href="https://arxiv.org/abs/2109.13488">Towards Rotation Invariance in Object Detection</a> was accepted at ICCV 2021.</li>
                <li><b>July 2020</b>: <b>Internship</b> - Joined Akasha Imaging (now part of <a target="_blank" href="https://www.intrinsic.ai/">Intrinsic</a>) as a Computer Vision and Robotics researcher.</li>
                
            </ul> -->
            
        <h2>Research</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle"  class="comp-teasers">
              <img src="images/tokasaurus.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://scalingintelligence.stanford.edu/blogs/tokasaurus">
                  <papertitle>Tokasaurus: An LLM Inference Engine for High-Throughput Workloads</papertitle>
              </a>
              <br>
              Jordan Juravsky,
              Ayush Chakravarthy,
              Sabri Eyuboglu,
              Ryan Ehrlich,
              <strong>Bradley Brown</strong>,
              Joseph Shetaye,
              Christopher R&eacute;,
              Azalia Mirhoseini
              <br>
              [
              <a target="_blank" href="https://scalingintelligence.stanford.edu/blogs/tokasaurus">Blog Post</a> /
              <a target="_blank" href="https://github.com/ScalingIntelligence/tokasaurus">Code</a> 
              ]
              <!-- TODO: add video -->
              <br>
              <p class="abstract">
                LLM inference engine optimized for throughput-intensive workloads. 
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/tokasaurus.png" alt="hpp" style="border-style: none" width="300">
              </div>
              <!-- <p class="abstract">
                Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.</p>
            </td> -->
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle"  class="comp-teasers">
              <img src="images/codemonkeys.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2501.14723">
                  <papertitle>CodeMonkeys: Scaling Test-Time Compute for Software Engineering</papertitle>
              </a>
              <br>
              Ryan Ehrlich<sup>*</sup>,
              <strong>Bradley Brown<sup>*</sup></strong>,
              Jordan Juravsky<sup>*</sup>,
              Ronald Clark,
              Christopher R&eacute;,
              Azalia Mirhoseini
              <br>
              [
              <a target="_blank" href="https://arxiv.org/abs/2501.14723">Paper</a> /
              <a target="_blank" href="https://github.com/ScalingIntelligence/codemonkeys">Code</a> /
              <a target="_blank" href="https://github.com/swe-bench/experiments/pull/171">Trajectories</a> /
              <a target="_blank" href="https://scalingintelligence.stanford.edu/blogs/codemonkeys/">Blog Post (Monkey SWE, Monkey Do)</a>
              ]
              <!-- TODO: add video -->
              <br>
              <p class="abstract">
                Building a system for solving SWE-bench issues that is explicitly designed for scaling test-time compute.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/codemonkeys.png" alt="hpp" style="border-style: none" width="300">
              </div>
              <!-- <p class="abstract">
                Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.</p>
            </td> -->
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle"  class="comp-teasers">
              <img src="images/large_language_monkeys.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2407.21787">
                  <papertitle>Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</papertitle>
              </a>
              <br>
              <strong>Bradley Brown<sup>*</sup></strong>,
              Jordan Juravsky<sup>*</sup>,
              Ryan Ehrlich<sup>*</sup>,
              Ronald Clark,
              Quoc V. Le,
              Christopher R&eacute;,
              Azalia Mirhoseini
              <br>
              <em>Preprint</em>.
              <br>
              [
              <a target="_blank" href="https://arxiv.org/abs/2407.21787">Paper</a> /
              <a target="_blank" href="https://github.com/ScalingIntelligence/large_language_monkeys">Code</a> /
              <a target="_blank" href="https://huggingface.co/datasets/ScalingIntelligence/monkey_business">Dataset</a> /
              <a target="_blank" href="https://scalingintelligence.stanford.edu/blogs/monkeys/">Blog Post</a>
              ]
              <!-- TODO: add video -->
              <br>
              <p class="abstract">
                Demonstrating that increasing the amount of inference compute through repeated sampling leads to large improvements in coverage - the fraction of problems solved by any attempt - across a variety tasks, models, and sample budgets. This makes it possible, and sometimes cost-effective, to amplify weaker models with many samples and outperform single attempts from more capable models.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/large_language_monkeys.png" alt="hpp" style="border-style: none" width="300">
              </div>
              <!-- <p class="abstract">
                Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.</p>
            </td> -->
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle"  class="comp-teasers">
              <img src="images/hydragen.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2402.05099">
                  <papertitle>Hydragen: High-Throughput LLM Inference with Shared Prefixes</papertitle>
              </a>
              <br>
              Jordan Juravsky<sup>*</sup>,
              <strong>Bradley Brown<sup>*</sup></strong>,
              Ryan Ehrlich<sup>*</sup>,
              Daniel Y. Fu,
              Christopher R&eacute;,
              Azalia Mirhoseini
              <br>
              <em>Preprint</em>.
              <br>
              [
              <a target="_blank" href="https://arxiv.org/abs/2402.05099">Paper</a> /
              <a target="_blank" href="https://github.com/ScalingIntelligence/hydragen">Code</a>
              ]
              <!-- TODO: add video -->
              <br>
              <p class="abstract">
                Introducing an exact, simple (no custom CUDA) implementation of attention that can accelerate LLM throughput by over 30x for problems containing shared prefixes and large batch sizes.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/hydragen.png" alt="hpp" style="border-style: none" width="300">
              </div>
              <!-- <p class="abstract">
                Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as virtual reality and robotics simulation. 
                Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. 
                We leverage Latent Diffusion Models that have been successfully utilized for efficient high-quality 2D content creation. 
                We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. 
                To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of latent representations. 
                A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline. We achieve a substantial improvement over existing state- of-the-art scene generation models. 
                Additionally, we show how NeuralField-LDM can be used for a variety of 3D content creation applications, including conditional scene generation, scene inpainting and scene style manipulation.</p>
            </td> -->
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <video alt="hpp" style="border-style: none" width="200" controls muted loop autoplay>
                <source src="images/nfldm_teaser.mp4" type="video/mp4">
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2304.09787">
                  <papertitle>NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models</papertitle>
              </a>
              <br>
              Seung Wook Kim<sup>*</sup>,
              <strong>Bradley Brown<sup>*</sup></strong>,
              Kangxue Yin, 
              Karsten Kreis, 
              Katja Schwarz, 
              Daiqing Li, 
              Robin Rombach, 
              Antonio Torralba, 
              Sanja Fidler
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2023</em>. 
              <br>
              [
              <a target="_blank" href="https://research.nvidia.com/labs/toronto-ai/NFLDM/">Project Page</a> /
              <a target="_blank" href="https://arxiv.org/abs/2304.09787">Paper</a>
              ]
              <!-- TODO: add video -->
              <br>
              <p class="abstract">
                Building a generative model of open-world 3D scenes trained on real-world in-the-wild data.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <video class="phone-teasers" alt="hpp" style="border-style: none" width="300" controls muted loop autoplay>
                  <source src="images/nfldm_teaser.mp4" type="video/mp4">
                </video>
              </div>
              <!-- <p class="abstract">
                Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as virtual reality and robotics simulation. 
                Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. 
                We leverage Latent Diffusion Models that have been successfully utilized for efficient high-quality 2D content creation. 
                We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. 
                To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of latent representations. 
                A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline. We achieve a substantial improvement over existing state- of-the-art scene generation models. 
                Additionally, we show how NeuralField-LDM can be used for a variety of 3D content creation applications, including conditional scene generation, scene inpainting and scene style manipulation.</p>
            </td> -->
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle"  class="comp-teasers">
              <img src="images/uom_box_plots.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2207.02862">
                  <papertitle>Verifying the Union of Manifolds Hypothesis for Image Data</papertitle>
              </a>
              <br>
              <strong>Bradley C.A. Brown</strong>,
              Anthony L. Caterini,
              Brendan Leigh Ross,
              Jesse C. Cresswell,
              Gabriel Loaiza-Ganem
              <br>
              <em>International Conference on Learning Representations (ICLR) 2023</em>. 
              <br>
              [<a target="_blank" href="https://arxiv.org/abs/2207.02862">Paper</a> / 
              <a target="_blank" href="https://iclr.cc/virtual/2023/poster/11032">Video</a> /
              <a target="_blank" href="https://github.com/layer6ai-labs/UoMH">Code</a>]
              <br>
              <p class="abstract">
                Extending the manifold hypothesis to support natural image data lying on a union of manifolds with varying intrinsic dimension. 
                Show increased performance in generative modelling and image classification tasks by designing models with an inductive bias for this structure.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/uom_box_plots.png" alt="hpp" style="border-style: none" width="300">
              </div>
            </td>
              <!-- <p class="abstract">
                The manifold hypothesis states that low-dimensional manifold structure exists in high-dimensional data, which is strongly supported by the success of deep learning in processing such data. 
                However, we argue here that the manifold hypothesis is incomplete, as it does not allow any variation in the intrinsic dimensionality of different sub-regions of the data space. We thus posit the union of manifold hypothesis, which states that high-dimensional data of interest comes from a union of disjoint manifolds; this allows intrinsic dimensionality to vary. 
                We empirically verify this hypothesis on image datasets using a standard estimator of intrinsic dimensionality, and also demonstrate an improvement in classification performance derived from this hypothesis. 
                We hope our work will encourage the community to further explore the benefits of considering the union of manifolds structure in data.</p>
            </td> -->
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle"  class="comp-teasers">
              <img src="images/biased_examples.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://openreview.net/forum?id=GJhsHNKm7kj">
                  <papertitle>Language Models Inversely Scale on Piecewise Function Evaluation with Biased Examples</papertitle>
              </a>
              <br>
              Jordan Juravsky<sup>*</sup>,
              <strong>Bradley Brown<sup>*</sup></strong>,
              Atif Mahmud<sup>*</sup>,
              Ryan Ehrlich<sup>*</sup>,
              Wais Shahbaz<sup>*</sup>
              <br>
              <em>Tiny Paper at the International Conference on Learning Representations (ICLR) 2023</em>. 
              <br>
              [<a target="_blank" href="https://openreview.net/forum?id=GJhsHNKm7kj">Paper</a>]
              <br>
              <p class="abstract">
                Demonstrating that large language models (LLMs) can be misled by providing them
                with factually correct, but unrepresentative/biased examples, in the context of
                integer-to-integer piecewise functions.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/biased_examples.png" alt="hpp" style="border-style: none" width="300">
              </div>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/reg_figure.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2211.13239">
                  <papertitle>Relating Regularization and Generalization through the Intrinsic Dimension of Activations</papertitle>
              </a>
              <br>
              <strong>Bradley C.A. Brown</strong>,
              Jordan Juravsky,
              Anthony L. Caterini,
              Gabriel Loaiza-Ganem
              <br>
              <em>NeurIPS 2022 workshops: <a target="_blank" href="https://opt-ml.org/">OPT 2022</a> and <a target="_blank" href="https://hity-workshop.github.io/NeurIPS2022/">HITY 2022</a></em>. 
              <br>
              [
              <a target="_blank" href="https://arxiv.org/abs/2211.13239">Paper</a> /
              <a target="_blank" href="https://github.com/BradleyBrown19/act_ids">Code</a>
              ]
              <br>
              <p class="abstract">
                Investigating how the intrinsic dimension of activations in deep neural networks are affected by regularization, correlated with improved validation performance and are coupled with the effects of sudden generalization (grokking).
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/reg_figure.png" alt="hpp" style="border-style: none" width="300">
              </div>
            </td>
              <!-- <p class="abstract">
                Given a pair of models with similar training set performance, it is natural to assume that the model that possesses simpler internal representations would exhibit better generalization. 
                In this work, we provide empirical evidence for this intuition through an analysis of the intrinsic dimension (ID) of model activations, which can be thought of as the minimal number of factors of variation in the model's representation of the data. 
                First, we show that common regularization techniques uniformly decrease the last-layer ID (LLID) of validation set activations for image classification models and show how this strongly affects model generalization performance. 
                We also investigate how excessive regularization decreases a model's ability to extract features from data in earlier layers, leading to a negative effect on validation accuracy even while LLID continues to decrease and training accuracy remains near-perfect. 
                Finally, we examine the LLID over the course of training of models that exhibit grokking. We observe that well after training accuracy saturates, when models "grok" and validation accuracy suddenly improves from random to perfect, there is a co-occurent sudden drop in LLID, thus providing more insight into the dynamics of sudden generalization.</p>
            </td> -->
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/recsys.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3556702.3556844">
                  <papertitle>Session-based Recommendation with Transformers</papertitle>
              </a>
              <br>
              Yichao Lu,
              Zhaolin Gao, 
              Zhaoyue Cheng,
              Jianing Sun,
              <strong>Bradley Brown</strong>,
              Guangwei Yu,
              Anson Wong,
              Felipe Pérez,
              Maksims Volkovs
              <br>
              <em>Proceedings of the <a target="_blank" href="https://recsys.acm.org/recsys22/challenge/">Recommender Systems Challenge 2022</a></em>. 
              <br>
              
              [<a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3556702.3556844">Paper</a>]
              <br>
              <p class="abstract">
                Leveraging transformers and self-supervised learning techniques to achieve 2/300+ teams on the RecSys session-based recommendation system challenge.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/recsys.png" alt="hpp" style="border-style: none" width="300">
              </div>
              <!-- <p class="abstract">
                Large item catalogs and constantly changing preference trends make recommendations a critically important component of every fashion e-commerce platform. 
                However, since most users browse anonymously, historical preference data is rarely available and recommendations have to be made using only information from within the session. 
                In the 2022 ACM RecSys challenge, Dressipi released a dataset with 1.1 million online retail sessions in the fashion domain that span an 18-month period. 
                The goal is to predict the item purchased at the end of each session. To simulate a common production scenario all sessions are anonymous and no previous user preference information is available.
                In this paper, we present our approach to this challenge. We leverage the Transformer architecture with two different learning objectives inspired by the self-supervised learning techniques to improve generalization. 
                Our team, LAYER 6, achieves strong results placing 2’nd on the final leaderboard out of over 300 teams.</p> -->
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/good_rotations.gif" alt="kts" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2109.13488">
                <papertitle>Towards Rotation Invariance in Object Detection</papertitle>
              </a>
              <br>
              Agastya Kalra,
              Guy Stoppi,
              <strong>Bradley Brown</strong>,
              Rishav Agarwal,
              Achuta Kadambi
              <br>
              <em>International Conference on Computer Vision (ICCV) 2021</em>. 
              <br>
              [
              <a target="_blank" href="https://arxiv.org/abs/2109.13488">Paper</a> /
              <a target="_blank" href="https://www.youtube.com/watch?v=VI_4EmgLb8s">Video</a> /
              <a target="_blank" href="https://github.com/akasha-imaging/ICCV2021">Code</a> ]
              <br>
              <p class="abstract">
                Proposing a mathematically sound rotation augmentation scheme and loss modification for object detection models that leads to better rotation invariance/equivariance.
              </p>
              <div class="phone-teasers" style="display: flex; justify-content: center; margin-top: 1em;">
                <img class="phone-teasers" src="images/good_rotations.gif" alt="hpp" style="border-style: none" width="200">
              </div>
              <!-- <p class="abstract">
                Rotation augmentations generally improve a model's invariance/equivariance to rotation - except in object detection. 
                In object detection the shape is not known, therefore rotation creates a label ambiguity. 
                We show that the de-facto method for bounding box label rotation, the Largest Box Method, creates very large labels, leading to poor performance and in many cases worse performance than using no rotation at all. 
                We propose a new method of rotation augmentation that can be implemented in a few lines of code. 
               First, we create a differentiable approximation of label accuracy and show that axis-aligning the bounding box around an ellipse is optimal. 
               We then introduce Rotation Uncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the labels. 
               On five different datasets (including COCO, PascalVOC, and Transparent Object Bin Picking), this approach improves the rotational invariance of both one-stage and two-stage architectures when measured with AP, AP50, and AP75. 
              </p> -->
            </td>
          </tr>

          

      </td>
    </tr>
  </table>
  Template from <a target="_blank" href="https://rosewang2008.github.io/">this website</a>, adapted from <a target="_blank" href="https://jonbarron.info/">this website</a>.
</body>

</html>
